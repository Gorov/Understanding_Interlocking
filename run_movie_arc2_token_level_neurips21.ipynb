{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np \n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# set seed\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from movie_data_utils import get_movie_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_loss_batch(z, percentage, mask=None):\n",
    "    \"\"\"\n",
    "    Compute regularization loss, based on a given rationale sequence\n",
    "\n",
    "    Inputs:\n",
    "        z -- torch variable, \"binary\" rationale, (batch_size, sequence_length)\n",
    "        percentage -- the percentage of words to keep\n",
    "    Outputs:\n",
    "        a loss value that contains two parts:\n",
    "        continuity_loss --  \\sum_{i} | z_{i-1} - z_{i} |\n",
    "        sparsity_loss -- |mean(z_{i}) - percent|\n",
    "    \"\"\"\n",
    "\n",
    "    # (batch_size,)\n",
    "    if mask is not None:\n",
    "        mask_z = z * mask\n",
    "        seq_lengths = torch.sum(mask, dim=1)\n",
    "    else:\n",
    "        mask_z = z\n",
    "        seq_lengths = torch.sum(z - z + 1.0, dim=1)\n",
    "\n",
    "    mask_z_ = torch.cat([mask_z[:, 1:], mask_z[:, -1:]], dim=-1)\n",
    "\n",
    "    continuity_loss = torch.sum(torch.abs(mask_z - mask_z_), dim=-1) / seq_lengths #(batch_size,)\n",
    "    sparsity_loss = torch.abs(torch.sum(mask_z, dim=-1) / seq_lengths - percentage)  #(batch_size,)\n",
    "\n",
    "    return continuity_loss, sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = list(range(9))\n",
    "# print(a[2:])\n",
    "# print(a[-2:])\n",
    "# print(a[:-2])\n",
    "\n",
    "# a = torch.rand(3,4)\n",
    "# print(a)\n",
    "# _, z_index = torch.topk(a, 2)\n",
    "# print(z_index.size())\n",
    "# print(z_index+1)\n",
    "# print(torch.cat([z_index, z_index+1], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colored import fg, attr, bg\n",
    "\n",
    "def display_example(vocab, x, z=None, threshold=0.05):\n",
    "\n",
    "    # apply threshold\n",
    "    condition = z >= threshold\n",
    "    for word_index, display_flag in zip(x, condition):\n",
    "        if word_index == 1:\n",
    "            continue\n",
    "        word = vocab.itos[word_index]\n",
    "        if display_flag:\n",
    "            output_word = \"%s %s%s\" %(fg(1), word, attr(0))\n",
    "            sys.stdout.write(output_word)\n",
    "        else:\n",
    "            sys.stdout.write(\" \" + word)\n",
    "    print('')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def _eval_att_rationale(token_select, mask, z, ratio=0.15):\n",
    "    _, z_index = torch.topk(token_select, int(mask.size(1) * ratio))\n",
    "    pred_z = torch.zeros_like(mask)\n",
    "    pred_z.scatter_(1,z_index,1)\n",
    "    \n",
    "    prec = torch.sum(pred_z * z, dim=1) / torch.sum(pred_z, dim=1)\n",
    "    rec = torch.sum(pred_z * z, dim=1) / (torch.sum(z, dim=1) + 1e-6)\n",
    "    \n",
    "    return prec, rec, pred_z\n",
    "\n",
    "def _eval_att_rationale_topk(token_select, mask, z, topk=10):\n",
    "    _, z_index = torch.topk(token_select, topk)\n",
    "    pred_z = torch.zeros_like(mask)\n",
    "    pred_z.scatter_(1,z_index,1)\n",
    "    \n",
    "    prec = torch.sum(pred_z * z, dim=1) / torch.sum(pred_z, dim=1)\n",
    "    rec = torch.sum(pred_z * z, dim=1) / (torch.sum(z, dim=1) + 1e-6)\n",
    "    \n",
    "    return prec, rec, pred_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = sys.argv[1]\n",
    "doc_dir = sys.argv[2]\n",
    "vocab, D_tr, D_dev, D_te = get_movie_datasets(data_dir, doc_dir, max_seq_len=2200, max_sent_num=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceA2R(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args    \n",
    "        self.highlight_ratio = args.highlight_ratio\n",
    "        self.highlight_count = args.highlight_count\n",
    "        self.exploration_rate = args.exploration_rate\n",
    "        self.vocab_size = vocab.vectors.shape[0]\n",
    "        self.embedding_dim = vocab.vectors.shape[1]\n",
    "        \n",
    "        self.lambda_dl = 0.\n",
    "        self.lambda_linear = 1.0 # 0.8\n",
    "        self.lambda_sparsity = 1.0\n",
    "\n",
    "        # Word embedding from pretrained vectors\n",
    "        self.pred_embd = nn.Embedding.from_pretrained(vocab.vectors, freeze=True)                \n",
    "        \n",
    "        # Token-level rationale classifier (serve as a head)\n",
    "        self.attention_fc = nn.Linear(args.rnn_dim * 2, 1, bias=False)\n",
    "        \n",
    "        # Predictor RNN\n",
    "        self.pred_gru = nn.GRU(self.embedding_dim, args.rnn_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Task predictor \n",
    "        self.pred_fc = nn.Linear(args.rnn_dim * 2, args.num_classes)\n",
    "        self.pred_fc_hard = nn.Linear(args.rnn_dim * 2, args.num_classes)\n",
    "        \n",
    "\n",
    "    def pred_vars(self):\n",
    "        \"\"\"\n",
    "        Return the variables of the predictor.\n",
    "        \"\"\"\n",
    "        params = list(self.pred_embd.parameters()) + list(\n",
    "            self.pred_gru.parameters()) + list(\n",
    "            self.pred_fc.parameters()) + list(\n",
    "            self.pred_fc_hard.parameters()) + list(self.attention_fc.parameters())\n",
    "        \n",
    "        return params\n",
    "            \n",
    "    \n",
    "    def _transform_token_rationale(self, token_select, mask):\n",
    "        \"\"\"\n",
    "        Returns transformed sent masks.\n",
    "        token_select -- (batch_size, seq_len)\n",
    "        mask -- (batch_size, seq_len)\n",
    "        sent_mask -- (batch_size, seq_len, num_sentences)\n",
    "        \"\"\"\n",
    "        \n",
    "        z_prob_ = token_select\n",
    "        \n",
    "#         _, z_index = torch.topk(z_prob_, int(mask.size(1) * self.highlight_ratio))\n",
    "        _, z_index = torch.topk(z_prob_, int(self.highlight_count))\n",
    "        z_ = torch.zeros_like(mask)\n",
    "        z_.scatter_(1,z_index,1)\n",
    "        \n",
    "        z = z_.float()\n",
    "        \n",
    "        return z\n",
    "\n",
    "    \n",
    "    def _transform_bigram_rationale(self, token_select, mask):\n",
    "        \"\"\"\n",
    "        Returns transformed sent masks.\n",
    "        token_select -- (batch_size, seq_len)\n",
    "        mask -- (batch_size, seq_len)\n",
    "        sent_mask -- (batch_size, seq_len, num_sentences)\n",
    "        \"\"\"\n",
    "        \n",
    "        z1 = token_select[:, 1:]\n",
    "        \n",
    "        z_prob_ = token_select[:,:-1] + z1\n",
    "        \n",
    "#         _, z_index = torch.topk(z_prob_, int(mask.size(1) * self.highlight_ratio / 3))\n",
    "        _, z_index = torch.topk(z_prob_, int(self.highlight_count))\n",
    "        z1_index = z_index + 1\n",
    "        \n",
    "        z_ = torch.zeros_like(mask)\n",
    "        \n",
    "        z_index_trigram = torch.cat([z_index, z1_index], dim=1)\n",
    "        z_.scatter_(1,z_index_trigram,1)\n",
    "        \n",
    "        z = z_.float()\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def _transform_trigram_rationale(self, token_select, mask):\n",
    "        \"\"\"\n",
    "        Returns transformed sent masks.\n",
    "        token_select -- (batch_size, seq_len)\n",
    "        mask -- (batch_size, seq_len)\n",
    "        sent_mask -- (batch_size, seq_len, num_sentences)\n",
    "        \"\"\"\n",
    "        \n",
    "        z1 = token_select[:, 1:]\n",
    "        z2 = token_select[:, 2:]\n",
    "        \n",
    "        z_prob_ = token_select[:,:-2] + z1[:,:-1] + z2\n",
    "        \n",
    "        _, z_index = torch.topk(z_prob_, int(mask.size(1) * self.highlight_ratio / 3))\n",
    "        z1_index = z_index + 1\n",
    "        z2_index = z_index + 2\n",
    "        \n",
    "        z_ = torch.zeros_like(mask)\n",
    "        \n",
    "        z_index_trigram = torch.cat([z_index, z1_index, z2_index], dim=1)\n",
    "        z_.scatter_(1,z_index_trigram,1)\n",
    "        \n",
    "        z = z_.float()\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def _transform_5gram_rationale(self, token_select, mask):\n",
    "        \"\"\"\n",
    "        Returns transformed sent masks.\n",
    "        token_select -- (batch_size, seq_len)\n",
    "        mask -- (batch_size, seq_len)\n",
    "        sent_mask -- (batch_size, seq_len, num_sentences)\n",
    "        \"\"\"\n",
    "        \n",
    "        z1 = token_select[:, 1:]\n",
    "        z2 = token_select[:, 2:]\n",
    "        z3 = token_select[:, 3:]\n",
    "        z4 = token_select[:, 4:]\n",
    "        \n",
    "        \n",
    "        z_prob_ = token_select[:,:-4] + z1[:,:-3] + z2[:,:-2] + z3[:,:-1] + z4\n",
    "        \n",
    "        _, z_index = torch.topk(z_prob_, int(mask.size(1) * self.highlight_ratio / 3))\n",
    "        z1_index = z_index + 1\n",
    "        z2_index = z_index + 2\n",
    "        z3_index = z_index + 3\n",
    "        z4_index = z_index + 4\n",
    "        \n",
    "        z_ = torch.zeros_like(mask)\n",
    "        \n",
    "        z_index_trigram = torch.cat([z_index, z1_index, z2_index, z3_index, z4_index], dim=1)\n",
    "        z_.scatter_(1,z_index_trigram,1)\n",
    "        \n",
    "        z = z_.float()\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def _transform_sent_rationales_topk(self, token_select, mask, sent_mask, topk=3):\n",
    "        \"\"\"\n",
    "        Returns transformed sent masks.\n",
    "        token_select -- (batch_size, seq_len)\n",
    "        mask -- (batch_size, seq_len)\n",
    "        sent_mask -- (batch_size, seq_len, num_sentences)\n",
    "        \"\"\"\n",
    "        \n",
    "        # (batch_size, seq_len, num_sentences)\n",
    "        token_select_per_sentence = token_select.unsqueeze(-1) * mask.unsqueeze(-1) * sent_mask\n",
    "        \n",
    "        # (batch_size, num_sentences)\n",
    "        num_select_per_sentence = torch.sum(token_select_per_sentence, dim=1)\n",
    "        num_per_sentence = torch.sum(sent_mask, dim=1)\n",
    "        ratio_select_per_sentence = num_select_per_sentence / num_per_sentence\n",
    "        \n",
    "        sent_probs_ = F.softmax(num_select_per_sentence, dim=-1) #(batch_size, num_sent)\n",
    "#         sent_probs_ = F.softmax(ratio_select_per_sentence, dim=-1) #(batch_size, num_sent)\n",
    "#         sent_select = torch.max(sent_probs_, dim=-1)[1]\n",
    "    \n",
    "        _, sent_select = torch.topk(sent_probs_, topk, dim=-1)\n",
    "#         print(sent_select.size())\n",
    "#         print(sent_select)\n",
    "         \n",
    "        sent_rationale = self._one_hot(sent_select[:,0], depth=sent_mask.shape[-1])\n",
    "        \n",
    "        for i in range(1, topk):\n",
    "            sent_rationale_ = self._one_hot(sent_select[:,i], depth=sent_mask.shape[-1])\n",
    "            sent_rationale += sent_rationale_\n",
    "        \n",
    "        return sent_rationale\n",
    "        \n",
    "    \n",
    "    def forward(self, seq, mask, sent_mask, y):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            seq -- (batch_size, seq_length)\n",
    "            mask -- (batch_size, seq_length), input mask\n",
    "            sent_mask -- (batch_size, seq_length, num_sents), assume only three sentences per example\n",
    "        \"\"\"\n",
    "        LARGE_NEG = -1e9\n",
    "        \n",
    "        max_seq_len = seq.shape[1]\n",
    "        lens = mask.sum(axis=1) # (batch_size, )\n",
    "        mask_ = mask.unsqueeze(-1) # (batch_size, seq_length, 1)\n",
    "        \n",
    "        pred_embs = self.pred_embd(seq) # (batch_size, seq_len, embedding_dim)\n",
    "        # mask the embedding \n",
    "        pred_embs_ = pred_embs\n",
    "\n",
    "        pred_embs_ = pack_padded_sequence(pred_embs_, lengths=lens, batch_first=True, enforce_sorted=False)\n",
    "        pred_outs_, _ = self.pred_gru(pred_embs_)\n",
    "        # (batch_size, seq_len, rnn_size)\n",
    "        pred_outs = pad_packed_sequence(pred_outs_, batch_first=True, \n",
    "                                       total_length=max_seq_len, padding_value=0.)[0] \n",
    "        \n",
    "        # (batch_size, seq_len) \n",
    "        token_att_logits = self.attention_fc(pred_outs).squeeze(-1)\n",
    "\n",
    "        token_probs = F.softmax(token_att_logits + (1.-mask)*LARGE_NEG, dim=-1) #(batch_size, seq_len)\n",
    "        rationale_select = self._transform_bigram_rationale(token_probs, mask).detach()\n",
    "#         rationale_select = self._transform_token_rationale(token_probs, mask).detach()\n",
    "\n",
    "        rationale_ = token_probs.unsqueeze(-1) # (batch_size, seq_len, 1) \n",
    "        pred_out = torch.sum(pred_outs * rationale_ * mask_, dim=1)\n",
    "\n",
    "        # classification\n",
    "        pred_logits = self.pred_fc(pred_out)\n",
    "\n",
    "        # hard classifier\n",
    "        hard_rationale_ = rationale_select.unsqueeze(-1)\n",
    "                    \n",
    "        # mask the embedding \n",
    "        pred_embs_sent_ = pred_embs * hard_rationale_\n",
    "\n",
    "        pred_embs_sent_ = pack_padded_sequence(pred_embs_sent_, lengths=lens, batch_first=True, enforce_sorted=False)\n",
    "        pred_outs_sent_, _ = self.pred_gru(pred_embs_sent_)\n",
    "        # (batch_size, seq_len, rnn_size)\n",
    "        pred_outs_sent = pad_packed_sequence(pred_outs_sent_, batch_first=True, \n",
    "                                       total_length=max_seq_len, padding_value=0.)[0] \n",
    "        \n",
    "        batch_mask = (torch.sum(rationale_select, dim=-1) > 1.).float().unsqueeze(-1) # (batch_size, 1)\n",
    "        # mask pred_outs using rationale\n",
    "        pred_outs_sent = hard_rationale_ * pred_outs_sent + (1. - hard_rationale_) * LARGE_NEG\n",
    "        # max pooling along seq direction\n",
    "        pred_out_sent = torch.max(pred_outs_sent, dim=1)[0]\n",
    "\n",
    "        # classification\n",
    "        pred_out_sent = pred_out_sent * batch_mask\n",
    "        pred_logits_sent = self.pred_fc_hard(pred_out_sent)\n",
    "\n",
    "        return pred_logits, pred_logits_sent, token_probs, rationale_select\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy():\n",
    "    pass\n",
    "\n",
    "args = Dummy()\n",
    "args.rnn_dim = 50 # 100\n",
    "args.num_classes = 2\n",
    "args.gumbel_temp = 1.0 # 0.1\n",
    "\n",
    "args.lambda_smooth = 1e-3\n",
    "\n",
    "args.exploration_rate = 0.2\n",
    "args.highlight_ratio = 0.2\n",
    "args.highlight_count = 80\n",
    "\n",
    "# args.highlight_ratio = 0.05\n",
    "\n",
    "args.l2_decay = 1e-4\n",
    "\n",
    "model = SentenceA2R(vocab, args).cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _get_attention_continuity_loss(att, mask):\n",
    "    mask_att = att * mask\n",
    "    seq_lengths = torch.sum(mask, dim=1)\n",
    "\n",
    "    mask_att_ = torch.cat([mask_att[:, 1:], mask_att[:, -1:]], dim=-1)\n",
    "    continuity_loss = torch.sum(torch.abs(mask_att - mask_att_), dim=-1)\n",
    "#     continuity_loss = torch.sum(torch.abs(mask_att - mask_att_), dim=-1) / seq_lengths #(batch_size,)\n",
    "    continuity_loss = torch.mean(continuity_loss)\n",
    "    \n",
    "    return continuity_loss\n",
    "\n",
    "def _get_top_attention_continuity_loss(att, z, mask):\n",
    "    mask_att = att * mask\n",
    "    mask_z = z * mask\n",
    "    seq_lengths = torch.sum(mask, dim=1)\n",
    "\n",
    "    mask_att_ = torch.cat([mask_att[:, 1:], mask_att[:, -1:]], dim=-1)\n",
    "    mask_att2_ = torch.cat([mask_att[:, 0:1], mask_att[:, 0:-1]], dim=-1)\n",
    "    \n",
    "    continuity_loss_right_ = torch.sum(torch.abs(mask_att - mask_att_) * mask_z, dim=-1)\n",
    "    continuity_loss_left_ = torch.sum(torch.abs(mask_att - mask_att2_) * mask_z, dim=-1)\n",
    "    \n",
    "    continuity_loss = continuity_loss_right_ + continuity_loss_left_\n",
    "    \n",
    "    continuity_loss = torch.mean(continuity_loss)\n",
    "    \n",
    "    return continuity_loss\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# gen_optimizer = torch.optim.Adam(model.gen_vars() , lr=1e-3)\n",
    "# gen_optimizer = torch.optim.Adam(model.gen_vars() , lr=1e-3 * 0.1)\n",
    "pred_optimizer = torch.optim.Adam(model.pred_vars() , lr=1e-3)\n",
    "\n",
    "# D_tr_ = DataLoader(D_tr, batch_size=100, shuffle=True, num_workers=16)\n",
    "# D_dev_ = DataLoader(D_dev, batch_size=100, shuffle=False, num_workers=4)\n",
    "# D_te_ = DataLoader(D_te, batch_size=100, shuffle=False, num_workers=4)\n",
    "D_tr_ = DataLoader(D_tr, batch_size=20, shuffle=True, num_workers=16)\n",
    "D_dev_ = DataLoader(D_dev, batch_size=20, shuffle=False, num_workers=4)\n",
    "D_te_ = DataLoader(D_te, batch_size=20, shuffle=False, num_workers=4)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "switch_epoch = 0\n",
    "\n",
    "queue_length = 200\n",
    "history_rewards = deque(maxlen=queue_length)\n",
    "history_rewards.append(0.)\n",
    "\n",
    "for i_epoch in range(num_epochs):\n",
    "\n",
    "    print (\"================\")\n",
    "    print (\"epoch: %d\" % i_epoch)\n",
    "    print (\"================\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i_batch, data in enumerate(D_tr_):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         gen_optimizer.zero_grad()\n",
    "        pred_optimizer.zero_grad()\n",
    "        counter += 1\n",
    "\n",
    "        x = data[\"x\"].cuda()\n",
    "        mask = data[\"mask\"].cuda()\n",
    "        y = data[\"y\"].cuda()\n",
    "        sent_mask = data[\"sent_mask\"].transpose(1,2).cuda().contiguous()\n",
    "        \n",
    "        logits, logits_sent, token_select, rationale_select = model(x, mask, sent_mask, y)\n",
    "        \n",
    "#         continuity_loss = _get_attention_continuity_loss(token_select, mask)\n",
    "        continuity_loss = _get_top_attention_continuity_loss(token_select, rationale_select, mask)\n",
    "\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs_sent = torch.softmax(logits_sent, dim=-1)\n",
    "\n",
    "        consistency_loss = F.kl_div(torch.log(probs), \n",
    "                                    probs_sent, reduction='batchmean')\n",
    "\n",
    "        consistency_loss += F.kl_div(torch.log(probs_sent), \n",
    "                                    probs, reduction='batchmean')\n",
    "\n",
    "        sup_loss = F.cross_entropy(logits, y) + F.cross_entropy(logits_sent, y) + consistency_loss\n",
    "#         sup_loss += continuity_loss * .01\n",
    "#         sup_loss = F.cross_entropy(logits, y) + F.cross_entropy(logits_sent, y) + consistency_loss * 2.\n",
    "\n",
    "        sup_loss.backward()\n",
    "        pred_optimizer.step()\n",
    "\n",
    "        pred = torch.max(logits, 1)[1]\n",
    "        acc = (pred == y).sum().float() / y.shape[0]\n",
    "        \n",
    "        if i_epoch >= switch_epoch:\n",
    "            pred_sent = torch.max(logits_sent, 1)[1]\n",
    "            acc_sent = (pred_sent == y).sum().float() / y.shape[0]\n",
    "        \n",
    "        if i_batch % 10 == 0:\n",
    "#             print (\"Train batch: %d, loss: %.4f, acc: %.4f.\" % (i_batch, loss.item(), acc.item()))\n",
    "            if i_epoch < switch_epoch:\n",
    "                print (\"Train batch: %d, loss: %.4f, acc: %.4f.\" % (i_batch, loss.item(), acc.item()))\n",
    "            elif i_batch == 0 and i_epoch == 0:\n",
    "                pass\n",
    "            else:\n",
    "                print (\"Train batch: %d, sup loss: %.4f, consis_loss: %.4f, acc: %.4f, sent_acc: %.4f.\" % (i_batch, \n",
    "                                                                                sup_loss.item(), \n",
    "                                                                                consistency_loss.item(),\n",
    "                                                                                acc.item(), acc_sent.item()))\n",
    "                print(F.cross_entropy(logits, y).item(), F.cross_entropy(logits_sent, y).item(), \n",
    "                      consistency_loss.item(), continuity_loss.item())\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     display_example(vocab, x[0], data[\"sent_mask\"][0][0])\n",
    "    \n",
    "    model.eval()\n",
    "#     if i_epoch >= switch_epoch:\n",
    "#         display_example(vocab, x[0], token_select[0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred = 0.\n",
    "        correct_pred_sent = 0.\n",
    "        num_sample = 0\n",
    "        \n",
    "        dev_sparsity = 0.\n",
    "#         z_prec = {10:0., 15:0., 20:0, 25:0., 30:0.}\n",
    "#         z_rec = {10:0., 15:0., 20:0, 25:0., 30:0.}\n",
    "#         z_f1 = {10:0., 15:0., 20:0, 25:0., 30:0.}\n",
    "        \n",
    "        z_prec = {}\n",
    "        z_rec = {}\n",
    "        z_f1 = {}\n",
    "        for key in [0.01, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]:\n",
    "            z_prec[key] = 0.\n",
    "            z_rec[key] = 0.\n",
    "            z_f1[key] = 0.\n",
    "        \n",
    "        z_prec_sent = 0.\n",
    "        z_rec_sent = 0.\n",
    "        z_f1_sent = 0.\n",
    "        z_total = 0.\n",
    "        \n",
    "        for i_batch, data in enumerate(D_dev_):\n",
    "            x = data[\"x\"].cuda()\n",
    "            mask = data[\"mask\"].cuda()\n",
    "            y = data[\"y\"].cuda()\n",
    "            sent_mask = data[\"sent_mask\"].transpose(1,2).cuda().contiguous()\n",
    "            z = data[\"z\"].cuda()\n",
    "                                 \n",
    "            if i_epoch < switch_epoch:\n",
    "                logits, sent_select, _, _, _ = model(x, mask, sent_mask, y)\n",
    "            else:\n",
    "                logits, logits_sent, token_select, rationale_select = model(x, mask, sent_mask, y)\n",
    "                    \n",
    "            pred = torch.max(logits, 1)[1]\n",
    "            correct_pred += (pred == y).sum().float()\n",
    "            if i_epoch >= switch_epoch:\n",
    "                pred_sent = torch.max(logits_sent, 1)[1]\n",
    "                correct_pred_sent += (pred_sent == y).sum().float()\n",
    "            num_sample += y.shape[0]\n",
    "            \n",
    "#             if i_batch == 0:\n",
    "#                 total_sent_select = torch.sum(sent_select, dim=0)\n",
    "#             else:\n",
    "#                 total_sent_select += torch.sum(sent_select, dim=0)\n",
    "                \n",
    "            if i_epoch >= switch_epoch:\n",
    "                mask_z = rationale_select * mask\n",
    "                seq_lengths = torch.sum(mask, dim=1)\n",
    "\n",
    "#                 sparsity_loss = torch.sum(rationale_select, dim=-1) / mask.size(1) #(batch_size,)\n",
    "                sparsity_loss = torch.sum(mask_z, dim=-1) / seq_lengths  #(batch_size,)\n",
    "                dev_sparsity += torch.sum(sparsity_loss).cpu().item()\n",
    "                \n",
    "            for key in z_prec.keys():\n",
    "#                 z_res_p, z_res_r, pred_z = _eval_att_rationale_topk(token_select, mask, z, topk=key)\n",
    "                z_res_p, z_res_r, pred_z = _eval_att_rationale(token_select, mask, z, ratio=key)\n",
    "                z_prec[key] += torch.sum(z_res_p)\n",
    "                z_rec[key] += torch.sum(z_res_r)\n",
    "                z_f1[key] += torch.sum(z_res_p * z_res_r * 2 / (z_res_p + z_res_r + 1e-6))\n",
    "                \n",
    "            z_total += z_res_p.size(0)\n",
    "                \n",
    "#             sent_rationale = sent_mask * sent_select.unsqueeze(1) # (batch_size, seq_len, num_sentences)\n",
    "#             sent_rationale = torch.sum(sent_rationale, dim=-1) # (batch_size, seq_len)\n",
    "            sent_rationale = rationale_select\n",
    "            \n",
    "            prec = torch.sum(sent_rationale * z, dim=1) / torch.sum(sent_rationale, dim=1)\n",
    "            rec = torch.sum(sent_rationale * z, dim=1) / (torch.sum(z, dim=1) + 1e-6)\n",
    "            \n",
    "            z_prec_sent += torch.sum(prec)\n",
    "            z_rec_sent += torch.sum(rec)\n",
    "            z_f1_sent += torch.sum(prec * rec * 2 / (prec + rec + 1e-6))\n",
    "           \n",
    "        # print (\"Dev acc: %.4f\" % (correct_pred / num_sample))\n",
    "        if i_epoch < switch_epoch:\n",
    "#             print (\"Dev acc: %.4f\" % (correct_pred / num_sample))\n",
    "            print (\"Dev acc: %.4f sent-level: %.4f\" % (correct_pred / num_sample, \n",
    "                                                       correct_pred_sent / num_sample))\n",
    "        else:\n",
    "            print (\"Dev acc: %.4f sent-level: %.4f\" % (correct_pred / num_sample, \n",
    "                                                       correct_pred_sent / num_sample))\n",
    "            \n",
    "        for key in z_prec.keys():\n",
    "            print(\"Top-%.4f: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (key, \n",
    "                                                                      z_prec[key] / z_total, \n",
    "                                                                      z_rec[key] / z_total, \n",
    "                                                                      z_f1[key] / z_total))\n",
    "        print(\"Sent-level: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (z_prec_sent / z_total, \n",
    "                                                                      z_rec_sent / z_total, \n",
    "                                                                      z_f1_sent / z_total))\n",
    "        \n",
    "        if i_epoch >= switch_epoch:\n",
    "            print (\"Token sparisity: %.4f\" % (dev_sparsity / num_sample)) \n",
    "            \n",
    "        correct_pred = 0.\n",
    "        correct_pred_sent = 0.\n",
    "        num_sample = 0\n",
    "        \n",
    "        dev_sparsity = 0.\n",
    "        z_prec = {}\n",
    "        z_rec = {}\n",
    "        z_f1 = {}\n",
    "        for key in [0.01, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]:\n",
    "            z_prec[key] = 0.\n",
    "            z_rec[key] = 0.\n",
    "            z_f1[key] = 0.\n",
    "        z_prec_sent = 0.\n",
    "        z_rec_sent = 0.\n",
    "        z_f1_sent = 0.\n",
    "        z_total = 0.\n",
    "            \n",
    "        \n",
    "        for i_batch, data in enumerate(D_te_):\n",
    "            x = data[\"x\"].cuda()\n",
    "            mask = data[\"mask\"].cuda()\n",
    "            y = data[\"y\"].cuda()\n",
    "            sent_mask = data[\"sent_mask\"].transpose(1,2).cuda().contiguous()\n",
    "            z = data[\"z\"].cuda()\n",
    "                                 \n",
    "            logits, logits_sent, token_select, rationale_select = model(x, mask, sent_mask, y)\n",
    "                    \n",
    "            pred = torch.max(logits, 1)[1]\n",
    "            correct_pred += (pred == y).sum().float()\n",
    "            if i_epoch >= switch_epoch:\n",
    "                pred_sent = torch.max(logits_sent, 1)[1]\n",
    "                correct_pred_sent += (pred_sent == y).sum().float()\n",
    "            num_sample += y.shape[0]\n",
    "            \n",
    "#             if i_batch == 0:\n",
    "#                 total_sent_select = torch.sum(sent_select, dim=0)\n",
    "#             else:\n",
    "#                 total_sent_select += torch.sum(sent_select, dim=0)\n",
    "                \n",
    "            if i_epoch >= switch_epoch:\n",
    "                mask_z = rationale_select * mask\n",
    "                seq_lengths = torch.sum(mask, dim=1)\n",
    "\n",
    "                sparsity_loss = torch.sum(mask_z, dim=-1) / seq_lengths  #(batch_size,)\n",
    "                dev_sparsity += torch.sum(sparsity_loss).cpu().item()\n",
    "                \n",
    "#             z_res_p, z_res_r, pred_z = _eval_att_rationale(token_select, mask, z, ratio=args.highlight_ratio)\n",
    "\n",
    "            for key in z_prec.keys():\n",
    "#                 z_res_p, z_res_r, pred_z = _eval_att_rationale_topk(token_select, mask, z, topk=key)\n",
    "                z_res_p, z_res_r, pred_z = _eval_att_rationale(token_select, mask, z, ratio=key)\n",
    "                z_prec[key] += torch.sum(z_res_p)\n",
    "                z_rec[key] += torch.sum(z_res_r)\n",
    "                z_f1[key] += torch.sum(z_res_p * z_res_r * 2 / (z_res_p + z_res_r + 1e-6))\n",
    "                \n",
    "            z_total += z_res_p.size(0)\n",
    "                \n",
    "#             sent_rationale = sent_mask * sent_select.unsqueeze(1) # (batch_size, seq_len, num_sentences)\n",
    "#             sent_rationale = torch.sum(sent_rationale, dim=-1) # (batch_size, seq_len)\n",
    "            sent_rationale = rationale_select\n",
    "            \n",
    "            prec = torch.sum(sent_rationale * z, dim=1) / torch.sum(sent_rationale, dim=1)\n",
    "            rec = torch.sum(sent_rationale * z, dim=1) / (torch.sum(z, dim=1) + 1e-6)\n",
    "            \n",
    "            z_prec_sent += torch.sum(prec)\n",
    "            z_rec_sent += torch.sum(rec)\n",
    "            z_f1_sent += torch.sum(prec * rec * 2 / (prec + rec + 1e-6))\n",
    "            \n",
    "#             z_total += z.size(0)\n",
    "            \n",
    "#             display_example(vocab, x[0], pred_z[0])\n",
    "            if i_batch == 0:\n",
    "                display_example(vocab, x[0], sent_rationale[0])\n",
    "#             display_example(vocab, x[0], z[:,2,:][0])\n",
    "\n",
    "           \n",
    "        if i_epoch < switch_epoch:\n",
    "            print (\"Test acc: %.4f\" % (correct_pred / num_sample))\n",
    "        else:\n",
    "            print (\"Test acc: %.4f sent-level: %.4f\" % (correct_pred / num_sample, \n",
    "                                                       correct_pred_sent / num_sample))\n",
    "        for key in z_prec.keys():\n",
    "            print(\"Top-%.4f: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (key, \n",
    "                                                                      z_prec[key] / z_total, \n",
    "                                                                      z_rec[key] / z_total, \n",
    "                                                                      z_f1[key] / z_total))\n",
    "        print(\"Sent-level: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (z_prec_sent / z_total, \n",
    "                                                                      z_rec_sent / z_total, \n",
    "                                                                      z_f1_sent / z_total))\n",
    "            \n",
    "        if i_epoch >= switch_epoch:\n",
    "            print (\"Token sparisity: %.4f\" % (dev_sparsity / num_sample)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
