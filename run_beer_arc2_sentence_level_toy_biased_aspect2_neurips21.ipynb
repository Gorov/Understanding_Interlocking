{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is sentence level Tao's Model.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np \n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# set seed\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from beer_data_utils_neurips21 import get_beer_datasets_biased\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bao_regularization_loss_batch(z, percentage, mask=None):\n",
    "    \"\"\"\n",
    "    Compute regularization loss, based on a given rationale sequence\n",
    "    Use Yujia's formulation\n",
    "\n",
    "    Inputs:\n",
    "        z -- torch variable, \"binary\" rationale, (batch_size, sequence_length)\n",
    "        percentage -- the percentage of words to keep\n",
    "    Outputs:\n",
    "        a loss value that contains two parts:\n",
    "        continuity_loss --  \\sum_{i} | z_{i-1} - z_{i} |\n",
    "        sparsity_loss -- |mean(z_{i}) - percent|\n",
    "    \"\"\"\n",
    "\n",
    "    # (batch_size,)\n",
    "    if mask is not None:\n",
    "        mask_z = z * mask\n",
    "        seq_lengths = torch.sum(mask, dim=1)\n",
    "    else:\n",
    "        mask_z = z\n",
    "        seq_lengths = torch.sum(z - z + 1.0, dim=1)\n",
    "\n",
    "    mask_z_ = torch.cat([mask_z[:, 1:], mask_z[:, -1:]], dim=-1)\n",
    "\n",
    "    continuity_loss = torch.sum(torch.abs(mask_z - mask_z_), dim=-1) / seq_lengths #(batch_size,)\n",
    "    sparsity_loss = torch.abs(torch.sum(mask_z, dim=-1) / seq_lengths - percentage)  #(batch_size,)\n",
    "\n",
    "    return continuity_loss, sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colored import fg, attr, bg\n",
    "\n",
    "def display_example(vocab, x, z=None, threshold=0.05):\n",
    "\n",
    "    # apply threshold\n",
    "    condition = z >= threshold\n",
    "    for word_index, display_flag in zip(x, condition):\n",
    "        if word_index == 1:\n",
    "            continue\n",
    "        word = vocab.itos[word_index]\n",
    "        if display_flag:\n",
    "            output_word = \"%s %s%s\" %(fg(1), word, attr(0))\n",
    "            sys.stdout.write(output_word)\n",
    "        else:\n",
    "            sys.stdout.write(\" \" + word)\n",
    "    print('')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def _eval_att_rationale(token_select, mask, z, ratio=0.15):\n",
    "    _, z_index = torch.topk(token_select, int(mask.size(1) * ratio))\n",
    "    pred_z = torch.zeros_like(mask)\n",
    "    pred_z.scatter_(1,z_index,1)\n",
    "    \n",
    "    prec = torch.sum(pred_z * z, dim=1) / torch.sum(pred_z, dim=1)\n",
    "    rec = torch.sum(pred_z * z, dim=1) / (torch.sum(z, dim=1) + 1e-6)\n",
    "    \n",
    "    return prec, rec, pred_z\n",
    "\n",
    "def _eval_att_rationale_topk(token_select, mask, z, topk=10):\n",
    "    _, z_index = torch.topk(token_select, topk)\n",
    "    pred_z = torch.zeros_like(mask)\n",
    "    pred_z.scatter_(1,z_index,1)\n",
    "    \n",
    "    prec = torch.sum(pred_z * z, dim=1) / torch.sum(pred_z, dim=1)\n",
    "    rec = torch.sum(pred_z * z, dim=1) / (torch.sum(z, dim=1) + 1e-6)\n",
    "    \n",
    "    return prec, rec, pred_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aspect = 2\n",
    "bias = 0.75\n",
    "# bias = 0.8\n",
    "\n",
    "data_dir = \"../data/beer_classification/aspect{}\".format(aspect)\n",
    "# vocab, D_tr, D_dev, D_te = get_beer_datasets(data_dir, max_seq_len=300, max_sent_num=50)\n",
    "vocab, D_tr, D_dev, D_te = get_beer_datasets_biased(data_dir, bias, max_seq_len=300, max_sent_num=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTao(nn.Module):\n",
    "    \"\"\"\n",
    "    Tao's model at sentence level (Toy setting).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args    \n",
    "        self.exploration_rate = args.exploration_rate\n",
    "        self.vocab_size = vocab.vectors.shape[0]\n",
    "        self.embedding_dim = vocab.vectors.shape[1]\n",
    "        \n",
    "        self.lambda_dl = 0.\n",
    "        self.lambda_linear = 1.0 # 0.8\n",
    "        self.lambda_sparsity = 1.0\n",
    "\n",
    "        # Word embedding from pretrained vectors\n",
    "        self.gen_embd = nn.Embedding.from_pretrained(vocab.vectors, freeze=False)\n",
    "        self.pred_embd = nn.Embedding.from_pretrained(vocab.vectors, freeze=False)                \n",
    "        \n",
    "#         self.linear_embd = nn.Embedding.from_pretrained(vocab.vectors, freeze=True)\n",
    "        \n",
    "        # Generator RNN\n",
    "        self.gen_gru = nn.GRU(self.embedding_dim, args.rnn_dim, batch_first=True, bidirectional=True)        \n",
    "\n",
    "        # Token-level rationale classifier (serve as a head)\n",
    "        self.gen_fc_token = nn.Linear(args.rnn_dim * 2, 1, bias=False)\n",
    "        \n",
    "#         self.gen_fc_token = nn.Sequential(\n",
    "#             nn.Linear(args.rnn_dim * 2, 1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "        # Sentence-level rationale classifier (serve as a head)\n",
    "        self.gen_fc = nn.Linear(args.rnn_dim * 2, 1, bias=False)\n",
    "        \n",
    "        # Predictor RNN\n",
    "        self.pred_gru = nn.GRU(self.embedding_dim, args.rnn_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Task predictor \n",
    "        self.pred_fc = nn.Linear(args.rnn_dim * 2, args.num_classes)\n",
    "        self.pred_fc_hard = nn.Linear(args.rnn_dim * 2, args.num_classes)\n",
    "        \n",
    "    def gen_vars(self):\n",
    "        \"\"\"\n",
    "        Return the variables of the generator.\n",
    "        \"\"\"\n",
    "        params = list(self.gen_embd.parameters()) + list(\n",
    "            self.gen_gru.parameters()) + list(\n",
    "            self.gen_fc.parameters()) + list(\n",
    "            self.gen_fc_token.parameters())\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def pred_vars(self):\n",
    "        \"\"\"\n",
    "        Return the variables of the predictor.\n",
    "        \"\"\"\n",
    "        params = list(self.pred_embd.parameters()) + list(\n",
    "            self.pred_gru.parameters()) + list(\n",
    "            self.pred_fc.parameters()) + list(self.pred_fc_hard.parameters())\n",
    "        \n",
    "        return params\n",
    "            \n",
    "    def _one_hot(self, idx, depth):\n",
    "        \"\"\"\n",
    "        Returns a one-hot tensor.\n",
    "        idx -- (batch_size, )\n",
    "        depth -- max number\n",
    "        \"\"\"\n",
    "        return torch.zeros(len(idx), depth, device=idx.device).scatter_(1, idx.unsqueeze(1), 1.)\n",
    "        \n",
    "    def _generate_rationales(self, z_prob_, num_samples=1):\n",
    "        '''\n",
    "        Input:\n",
    "            z_prob_ -- (num_rows, length, z_dim)\n",
    "        Output:\n",
    "            z -- (num_rows, length)\n",
    "        '''        \n",
    "        \n",
    "        # (num_rows, length)\n",
    "        log_probs = torch.log(z_prob_)\n",
    "        \n",
    "        # sample actions\n",
    "        if self.training:\n",
    "            z_ = torch.multinomial(z_prob_, 1, replacement=True)\n",
    "            neg_log_probs_ = - log_probs.gather(1, z_)\n",
    "            z = z_.squeeze(1) #.float()\n",
    "        else:\n",
    "            z_ = torch.max(z_prob_, dim=-1)[1]\n",
    "            neg_log_probs_ = - log_probs.gather(1, z_.unsqueeze(-1))\n",
    "            z = z_\n",
    "\n",
    "        # (num_rows, num_samples)\n",
    "        neg_log_probs = neg_log_probs_.squeeze(1)\n",
    "        \n",
    "        return z, neg_log_probs\n",
    "        \n",
    "    \n",
    "    def forward(self, seq, mask, sent_mask, y, train_part='generator'):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            seq -- (batch_size, seq_length)\n",
    "            mask -- (batch_size, seq_length), input mask\n",
    "            sent_mask -- (batch_size, seq_length, num_sents), assume only three sentences per example\n",
    "        \"\"\"\n",
    "        LARGE_NEG = -1e9\n",
    "        \n",
    "        max_seq_len = seq.shape[1]\n",
    "        lens = mask.sum(axis=1) # (batch_size, )\n",
    "        mask_ = mask.unsqueeze(-1) # (batch_size, seq_length, 1)\n",
    "        \n",
    "        sent_row_mask = (torch.sum(sent_mask, dim=1) > 1.).float() # (batch_size, num_sent)\n",
    "        \n",
    "        if train_part == \"generator\":\n",
    "            # Word embedding\n",
    "            gen_embs = self.gen_embd(seq) # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "            # Generator\n",
    "            gen_embs_ = pack_padded_sequence(gen_embs, lengths=lens, batch_first=True, enforce_sorted=False)\n",
    "            gen_outs_, _ = self.gen_gru(gen_embs_)\n",
    "            # (batch_size, seq_len, rnn_size)\n",
    "#             gen_outs = pad_packed_sequence(gen_outs_, batch_first=True, \n",
    "#                                            total_length=max_seq_len, padding_value=LARGE_NEG)[0]\n",
    "            gen_outs = pad_packed_sequence(gen_outs_, batch_first=True, \n",
    "                                           total_length=max_seq_len, padding_value=0.)[0]\n",
    "            \n",
    "            # Sentence-level pooling        \n",
    "            # (batch_size, seq_len, rnn_size, 1)\n",
    "            gen_outs_ = gen_outs.unsqueeze(-1)\n",
    "            # (batch_size, seq_len, 1, num_sentences)\n",
    "            sent_mask_ = sent_mask.unsqueeze(-2)\n",
    "            \n",
    "            # get representations per sentence\n",
    "            sent_repts_ = gen_outs_ * sent_mask_ + (1 - sent_mask_) * LARGE_NEG\n",
    "            # (batch_size, rnn_size, num_sentences)\n",
    "            sent_repts = torch.max(sent_repts_, dim=1)[0]\n",
    "            # (batch_size, num_sentences, rnn_size)\n",
    "            sent_repts = torch.transpose(sent_repts, 1, 2).contiguous()\n",
    "\n",
    "            # (batch_size, num_sentences) \n",
    "            sent_att_logits = self.gen_fc(sent_repts).squeeze(-1)\n",
    "            # (batch_size, num_sentences) in one-hot format\n",
    "            sent_probs = F.softmax(sent_att_logits * sent_row_mask + (1 - sent_row_mask) * LARGE_NEG, dim=-1)\n",
    "    \n",
    "            sent_probs_ = (1 - self.exploration_rate) * sent_probs + self.exploration_rate / sent_probs.size(-1)\n",
    "            sent_select, neg_log_probs = self._generate_rationales(sent_probs_)\n",
    "            sent_select = self._one_hot(sent_select, depth=sent_mask.shape[-1])\n",
    "        \n",
    "        elif train_part == \"classifiers\":\n",
    "            with torch.no_grad():\n",
    "                # Word embedding\n",
    "                gen_embs = self.gen_embd(seq) # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "                # Generator\n",
    "                gen_embs_ = pack_padded_sequence(gen_embs, lengths=lens, batch_first=True, enforce_sorted=False)\n",
    "                gen_outs_, _ = self.gen_gru(gen_embs_)\n",
    "                # (batch_size, seq_len, rnn_size)\n",
    "#                 gen_outs = pad_packed_sequence(gen_outs_, batch_first=True, \n",
    "#                                                total_length=max_seq_len, padding_value=LARGE_NEG)[0]\n",
    "                gen_outs = pad_packed_sequence(gen_outs_, batch_first=True, \n",
    "                                               total_length=max_seq_len, padding_value=0.)[0]\n",
    "\n",
    "                # Sentence-level pooling        \n",
    "                # (batch_size, seq_len, rnn_size, 1)\n",
    "                gen_outs_ = gen_outs.unsqueeze(-1)\n",
    "                # (batch_size, seq_len, 1, num_sentences)\n",
    "                sent_mask_ = sent_mask.unsqueeze(-2)\n",
    "\n",
    "                # get representations per sentence\n",
    "                sent_repts_ = gen_outs_ * sent_mask_ + (1 - sent_mask_) * LARGE_NEG\n",
    "                # (batch_size, rnn_size, num_sentences)\n",
    "                sent_repts = torch.max(sent_repts_, dim=1)[0]\n",
    "                # (batch_size, num_sentences, rnn_size)\n",
    "                sent_repts = torch.transpose(sent_repts, 1, 2).contiguous()\n",
    "\n",
    "                # (batch_size, num_sentences) \n",
    "                sent_att_logits = self.gen_fc(sent_repts).squeeze(-1)\n",
    "                # (batch_size, num_sentences) in one-hot format\n",
    "                sent_probs = F.softmax(sent_att_logits * sent_row_mask + (1 - sent_row_mask) * LARGE_NEG, dim=-1)\n",
    "\n",
    "                sent_probs_ = (1 - self.exploration_rate) * sent_probs + self.exploration_rate / sent_probs.size(-1)\n",
    "                sent_select, neg_log_probs = self._generate_rationales(sent_probs_)\n",
    "                sent_select = self._one_hot(sent_select, depth=sent_mask.shape[-1])\n",
    "\n",
    "                sent_select = sent_select.detach()\n",
    "                neg_log_probs = neg_log_probs.detach()\n",
    "\n",
    "        rationale_ = sent_probs.unsqueeze(-1) # (batch_size, num_sent, 1) \n",
    "\n",
    "        sent_rationale = sent_mask * sent_select.unsqueeze(1) # (batch_size, seq_len, num_sentences)\n",
    "        sent_rationale = torch.sum(sent_rationale, dim=-1) # (batch_size, seq_len)\n",
    "        sent_rationale_ = sent_rationale.unsqueeze(-1)  \n",
    "                    \n",
    "        # Predictor \n",
    "        # mask out non-rationale words\n",
    "        pred_embs = self.pred_embd(seq) # (batch_size, seq_len, embedding_dim)\n",
    "        # mask the embedding \n",
    "        pred_embs_ = pred_embs\n",
    "\n",
    "        pred_embs_ = pack_padded_sequence(pred_embs_, lengths=lens, batch_first=True, enforce_sorted=False)\n",
    "        pred_outs_, _ = self.pred_gru(pred_embs_)\n",
    "        # (batch_size, seq_len, rnn_size)\n",
    "        pred_outs = pad_packed_sequence(pred_outs_, batch_first=True, \n",
    "                                       total_length=max_seq_len, padding_value=0.)[0] \n",
    "\n",
    "        # Sentence-level pooling        \n",
    "        # (batch_size, seq_len, rnn_size, 1)\n",
    "        pred_outs_ = pred_outs.unsqueeze(-1)\n",
    "        # (batch_size, seq_len, 1, num_sentences)\n",
    "        sent_mask_ = sent_mask.unsqueeze(-2)\n",
    "\n",
    "        # get representations per sentence\n",
    "        pred_sent_repts_ = pred_outs_ * sent_mask_ + (1 - sent_mask_) * LARGE_NEG\n",
    "        # (batch_size, rnn_size, num_sentences)\n",
    "        pred_sent_repts = torch.max(pred_sent_repts_, dim=1)[0]\n",
    "        # (batch_size, num_sentences, rnn_size)\n",
    "        pred_sent_repts = torch.transpose(pred_sent_repts, 1, 2).contiguous()\n",
    "        \n",
    "        pred_out = torch.sum(pred_sent_repts * rationale_, dim=1)\n",
    "\n",
    "        # classification\n",
    "        pred_logits = self.pred_fc(pred_out)\n",
    "        \n",
    "        # mask the embedding \n",
    "        pred_embs_sent_ = pred_embs * sent_rationale_\n",
    "\n",
    "        pred_embs_sent_ = pack_padded_sequence(pred_embs_sent_, lengths=lens, batch_first=True, enforce_sorted=False)\n",
    "        pred_outs_sent_, _ = self.pred_gru(pred_embs_sent_)\n",
    "        # (batch_size, seq_len, rnn_size)\n",
    "        pred_outs_sent = pad_packed_sequence(pred_outs_sent_, batch_first=True, \n",
    "                                       total_length=max_seq_len, padding_value=0.)[0] \n",
    "        \n",
    "        batch_mask = (torch.sum(sent_rationale, dim=-1) > 1.).float().unsqueeze(-1) # (batch_size, 1)\n",
    "        # mask pred_outs using rationale\n",
    "        pred_outs_sent = sent_rationale_ * pred_outs_sent + (1. - sent_rationale_) * LARGE_NEG\n",
    "        # max pooling along seq direction\n",
    "        pred_out_sent = torch.max(pred_outs_sent, dim=1)[0]\n",
    "        \n",
    "#         pred_outs_sent = sent_rationale_ * pred_outs_sent\n",
    "#         pred_out_sent = torch.sum(pred_outs_sent, dim=1) / torch.sum(sent_rationale_, dim=1)\n",
    "\n",
    "        # classification\n",
    "        pred_out_sent = pred_out_sent * batch_mask\n",
    "        pred_logits_sent = self.pred_fc_hard(pred_out_sent)\n",
    "        \n",
    "        return pred_logits, pred_logits_sent, sent_probs, sent_select, neg_log_probs\n",
    "             \n",
    "        \n",
    "    def get_advantages(self, predict, label, baseline):\n",
    "        '''\n",
    "        Input:\n",
    "            z -- (batch_size, length)\n",
    "        '''\n",
    "            \n",
    "        with torch.no_grad():\n",
    "\n",
    "            e_loss = torch.tanh(F.cross_entropy(predict, label, reduction='none'))\n",
    "\n",
    "            rewards = - e_loss\n",
    "\n",
    "            advantages = rewards - baseline # (batch_size,)\n",
    "            advantages = Variable(advantages.data, requires_grad=False).to(predict.device)\n",
    "        \n",
    "        return advantages, rewards\n",
    "    \n",
    "    \n",
    "    def get_loss(self, predict, label, neg_log_probs, baseline):\n",
    "\n",
    "        reward_tuple = self.get_advantages(predict, label, baseline)\n",
    "        advantages, rewards = reward_tuple\n",
    "\n",
    "        # (batch_size, num_samples)\n",
    "        rl_loss = torch.sum(neg_log_probs * advantages) / neg_log_probs.size(0)\n",
    "#         rl_loss2 = torch.sum(neg_log_probs2 * advantages_flat_) / (neg_log_probs2.size(0) * neg_log_probs2.size(1))\n",
    "        \n",
    "        return rl_loss, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy():\n",
    "    pass\n",
    "\n",
    "args = Dummy()\n",
    "args.rnn_dim = 100\n",
    "args.num_classes = 2\n",
    "args.gumbel_temp = 1.0 # 0.1\n",
    "\n",
    "args.lambda_smooth = 1e-3\n",
    "\n",
    "args.exploration_rate = 0.2\n",
    "\n",
    "args.highlight_ratio = 0.05\n",
    "\n",
    "args.l2_decay = 1e-4\n",
    "\n",
    "model = SentenceTao(vocab, args).cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "# gen_optimizer = torch.optim.Adam(model.gen_vars() , lr=1e-3)\n",
    "gen_optimizer = torch.optim.Adam(model.gen_vars() , lr=1e-3 * 0.1)\n",
    "pred_optimizer = torch.optim.Adam(model.pred_vars() , lr=1e-3)\n",
    "\n",
    "D_tr_ = DataLoader(D_tr, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "D_dev_ = DataLoader(D_dev, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "D_te_ = DataLoader(D_te, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "switch_epoch = 0\n",
    "\n",
    "queue_length = 200\n",
    "history_rewards = deque(maxlen=queue_length)\n",
    "history_rewards.append(0.)\n",
    "\n",
    "for i_epoch in range(num_epochs):\n",
    "\n",
    "    print (\"================\")\n",
    "    print (\"epoch: %d\" % i_epoch)\n",
    "    print (\"================\")\n",
    "    \n",
    "    if i_epoch == switch_epoch:\n",
    "        print(\"SWITCH TO ADDING RL LOSS\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i_batch, data in enumerate(D_tr_):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "        gen_optimizer.zero_grad()\n",
    "        pred_optimizer.zero_grad()\n",
    "        counter += 1\n",
    "\n",
    "        x = data[\"x\"].cuda()\n",
    "        mask = data[\"mask\"].cuda()\n",
    "        y = data[\"y\"].cuda()\n",
    "        sent_mask = data[\"sent_mask\"].transpose(1,2).cuda().contiguous()\n",
    "        \n",
    "\n",
    "        if counter % 2 == 0:\n",
    "            # update the classifiers\n",
    "\n",
    "#             logits, sent_select, _ = model(x, mask, sent_mask, y, \"classifiers\", sent_select_policy='skew_z1')\n",
    "#             if i_epoch < switch_epoch:\n",
    "#                 logits, logits_sent, _, _, _  = model.forward_skew_classifier(x, mask, sent_mask, y)\n",
    "#                 loss = F.cross_entropy(logits, y) + F.cross_entropy(logits_sent, y)\n",
    "#             else:\n",
    "#                 logits, sent_select, _, _ = model(x, mask, sent_mask, y, \"classifiers\", sent_select_policy='gen')\n",
    "            logits, logits_sent, token_select, sent_select, _ = model(x, mask, sent_mask, y, \"classifiers\")\n",
    "\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            probs_sent = torch.softmax(logits_sent, dim=-1)\n",
    "\n",
    "            consistency_loss = F.kl_div(torch.log(probs), \n",
    "                                        probs_sent, reduction='batchmean')\n",
    "\n",
    "            consistency_loss += F.kl_div(torch.log(probs_sent), \n",
    "                                        probs, reduction='batchmean')\n",
    "\n",
    "            sup_loss = F.cross_entropy(logits, y) + F.cross_entropy(logits_sent, y) + consistency_loss\n",
    "#                 sup_loss = F.cross_entropy(logits, y) + F.cross_entropy(logits_sent, y) + consistency_loss * 2.\n",
    "\n",
    "            sup_loss.backward()\n",
    "            pred_optimizer.step()\n",
    "        else:            \n",
    "            logits, logits_sent, token_select, sent_select, neg_log_probs = model(x, mask, sent_mask, \n",
    "                                                                                  y, \"generator\")\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            probs_sent = torch.softmax(logits_sent, dim=-1)\n",
    "            consis_loss = 0.5 * torch.sum(F.kl_div(torch.log(probs), probs_sent, reduction='none'), dim=1)\n",
    "            consis_loss += 0.5 * torch.sum(F.kl_div(torch.log(probs_sent), probs, reduction='none'), dim=1)\n",
    "\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "            baseline = Variable(torch.FloatTensor([float(np.mean(history_rewards))])).to(neg_log_probs.device)\n",
    "            rl_loss, rewards = model.get_loss(logits_sent, y, neg_log_probs, baseline)\n",
    "\n",
    "            batch_reward = np.mean(rewards.cpu().data.numpy())\n",
    "            history_rewards.append(batch_reward)\n",
    "            \n",
    "#             if i_epoch < switch_epoch:\n",
    "#                 loss += 0. * rl_loss\n",
    "#             else:\n",
    "#                 loss += rl_loss\n",
    "\n",
    "            loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "        pred = torch.max(logits, 1)[1]\n",
    "        acc = (pred == y).sum().float() / y.shape[0]\n",
    "        \n",
    "        if i_epoch >= switch_epoch:\n",
    "            pred_sent = torch.max(logits_sent, 1)[1]\n",
    "            acc_sent = (pred_sent == y).sum().float() / y.shape[0]\n",
    "        \n",
    "        if i_batch % 10 == 0:\n",
    "#             print (\"Train batch: %d, loss: %.4f, acc: %.4f.\" % (i_batch, loss.item(), acc.item()))\n",
    "            if i_epoch < switch_epoch:\n",
    "                print (\"Train batch: %d, loss: %.4f, acc: %.4f.\" % (i_batch, loss.item(), acc.item()))\n",
    "            elif i_batch == 0 and i_epoch == 0:\n",
    "                pass\n",
    "            else:\n",
    "                print (\"Train batch: %d, sup loss: %.4f, rl loss: %.4f, consis_loss: %.4f, acc: %.4f, sent_acc: %.4f.\" % (i_batch, \n",
    "                                                                                sup_loss.item(), \n",
    "                                                                                loss.item(), \n",
    "                                                                                consistency_loss.item(),\n",
    "                                                                                acc.item(), acc_sent.item()))\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "    display_example(vocab, x[0], data[\"sent_mask\"][0][0])\n",
    "    \n",
    "    model.eval()\n",
    "#     if i_epoch >= switch_epoch:\n",
    "#         display_example(vocab, x[0], token_select[0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct_pred = 0.\n",
    "        correct_pred_sent = 0.\n",
    "        num_sample = 0\n",
    "        \n",
    "        dev_sparsity = 0.\n",
    "        \n",
    "        for i_batch, data in enumerate(D_dev_):\n",
    "            x = data[\"x\"].cuda()\n",
    "            mask = data[\"mask\"].cuda()\n",
    "            y = data[\"y\"].cuda()\n",
    "            sent_mask = data[\"sent_mask\"].transpose(1,2).cuda().contiguous()\n",
    "#             z = data[\"z\"].cuda()\n",
    "                                 \n",
    "            if i_epoch < switch_epoch:\n",
    "                logits, sent_select, _, _, _ = model(x, mask, sent_mask, y)\n",
    "            else:\n",
    "                logits, logits_sent, token_select, sent_select, _ = model(x, mask, sent_mask, y)\n",
    "                    \n",
    "            pred = torch.max(logits, 1)[1]\n",
    "            correct_pred += (pred == y).sum().float()\n",
    "            if i_epoch >= switch_epoch:\n",
    "                pred_sent = torch.max(logits_sent, 1)[1]\n",
    "                correct_pred_sent += (pred_sent == y).sum().float()\n",
    "            num_sample += y.shape[0]\n",
    "            \n",
    "#             if i_batch == 0:\n",
    "#                 total_sent_select = torch.sum(sent_select, dim=0)\n",
    "#             else:\n",
    "#                 total_sent_select += torch.sum(sent_select, dim=0)\n",
    "                \n",
    "#             if i_epoch >= switch_epoch:\n",
    "#                 mask_z = token_select * mask\n",
    "#                 seq_lengths = torch.sum(mask, dim=1)\n",
    "\n",
    "#                 sparsity_loss = torch.sum(mask_z, dim=-1) / seq_lengths  #(batch_size,)\n",
    "#                 dev_sparsity += torch.sum(sparsity_loss).cpu().item()\n",
    "           \n",
    "        # print (\"Dev acc: %.4f\" % (correct_pred / num_sample))\n",
    "        if i_epoch < switch_epoch:\n",
    "#             print (\"Dev acc: %.4f\" % (correct_pred / num_sample))\n",
    "            print (\"Dev acc: %.4f sent-level: %.4f\" % (correct_pred / num_sample, \n",
    "                                                       correct_pred_sent / num_sample))\n",
    "        else:\n",
    "            print (\"Dev acc: %.4f sent-level: %.4f\" % (correct_pred / num_sample, \n",
    "                                                       correct_pred_sent / num_sample))\n",
    "        if i_epoch >= switch_epoch:\n",
    "            print (\"Token sparisity: %.4f\" % (dev_sparsity / num_sample)) \n",
    "            \n",
    "        correct_pred = 0.\n",
    "        correct_pred_sent = 0.\n",
    "        num_sample = 0\n",
    "        \n",
    "        dev_sparsity = 0.\n",
    "#         z_prec = {10:0.,20:0}\n",
    "        z_prec = {10:0., 15:0., 20:0, 25:0., 30:0.}\n",
    "        z_rec = {10:0., 15:0., 20:0, 25:0., 30:0.}\n",
    "        z_f1 = {10:0., 15:0., 20:0, 25:0., 30:0.}\n",
    "#         z_prec_sent = 0.\n",
    "#         z_rec_sent = 0.\n",
    "#         z_f1_sent = 0.\n",
    "        z_total = 0.\n",
    "        z_prec_sent = {} # 0.\n",
    "        z_rec_sent = {} # 0.\n",
    "        z_f1_sent = {} # 0.\n",
    "        z_total_sent = {} #0.\n",
    "        \n",
    "        for z_idx in range(5):\n",
    "            z_prec_sent[z_idx] = 0.\n",
    "            z_rec_sent[z_idx] = 0.\n",
    "            z_f1_sent[z_idx] = 0.\n",
    "            z_total_sent[z_idx] = 0.\n",
    "            \n",
    "        z1_prec_sent = 0.\n",
    "        z1_rec_sent = 0.\n",
    "        z1_f1_sent = 0.\n",
    "        z1_total = 0.\n",
    "        \n",
    "        for i_batch, data in enumerate(D_te_):\n",
    "            x = data[\"x\"].cuda()\n",
    "            mask = data[\"mask\"].cuda()\n",
    "            y = data[\"y\"].cuda()\n",
    "            sent_mask = data[\"sent_mask\"].transpose(1,2).cuda().contiguous()\n",
    "            z = data[\"z\"].cuda()\n",
    "                                 \n",
    "            logits, logits_sent, token_select, sent_select, _ = model(x, mask, sent_mask, y)\n",
    "                    \n",
    "            pred = torch.max(logits, 1)[1]\n",
    "            correct_pred += (pred == y).sum().float()\n",
    "            if i_epoch >= switch_epoch:\n",
    "                pred_sent = torch.max(logits_sent, 1)[1]\n",
    "                correct_pred_sent += (pred_sent == y).sum().float()\n",
    "            num_sample += y.shape[0]\n",
    "            \n",
    "#             if i_batch == 0:\n",
    "#                 total_sent_select = torch.sum(sent_select, dim=0)\n",
    "#             else:\n",
    "#                 total_sent_select += torch.sum(sent_select, dim=0)\n",
    "                \n",
    "#             if i_epoch >= switch_epoch:\n",
    "#                 mask_z = token_select * mask\n",
    "#                 seq_lengths = torch.sum(mask, dim=1)\n",
    "\n",
    "#                 sparsity_loss = torch.sum(mask_z, dim=-1) / seq_lengths  #(batch_size,)\n",
    "#                 dev_sparsity += torch.sum(sparsity_loss).cpu().item()\n",
    "                \n",
    "#             z_res_p, z_res_r, pred_z = _eval_att_rationale(token_select, mask, z, ratio=args.highlight_ratio)\n",
    "#             for key in z_prec.keys():\n",
    "#                 z_res_p, z_res_r, pred_z = _eval_att_rationale_topk(token_select, mask, z[:,2,:], topk=key)\n",
    "#                 z_prec[key] += torch.sum(z_res_p)\n",
    "#                 z_rec[key] += torch.sum(z_res_r)\n",
    "#                 z_f1[key] += torch.sum(z_res_p * z_res_r * 2 / (z_res_p + z_res_r + 1e-6))\n",
    "                \n",
    "#             z_total += z_res_p.size(0)\n",
    "                \n",
    "            sent_rationale = sent_mask * sent_select.unsqueeze(1) # (batch_size, seq_len, num_sentences)\n",
    "            sent_rationale = torch.sum(sent_rationale, dim=-1) # (batch_size, seq_len)\n",
    "            \n",
    "            for z_idx in range(5):\n",
    "                prec = torch.sum(sent_rationale * z[:,z_idx,:], dim=1) / (torch.sum(sent_rationale, dim=1) + 1e-6)\n",
    "                rec = torch.sum(sent_rationale * z[:,z_idx,:], dim=1) / (torch.sum(z[:,z_idx,:], dim=1) + 1e-6)\n",
    "            \n",
    "                z_prec_sent[z_idx] += torch.sum(prec)\n",
    "                z_rec_sent[z_idx] += torch.sum(rec)\n",
    "                z_f1_sent[z_idx] += torch.sum(prec * rec * 2 / (prec + rec + 1e-6))\n",
    "            \n",
    "                z_total_sent[z_idx] += z.size(0)\n",
    "            \n",
    "#             prec = torch.sum(sent_rationale * z, dim=1) / torch.sum(sent_rationale, dim=1)\n",
    "#             rec = torch.sum(sent_rationale * z, dim=1) / (torch.sum(z, dim=1) + 1e-6)\n",
    "            \n",
    "#             z_prec_sent += torch.sum(prec)\n",
    "#             z_rec_sent += torch.sum(rec)\n",
    "#             z_f1_sent += torch.sum(prec * rec * 2 / (prec + rec + 1e-6))\n",
    "            \n",
    "#             z_total += z_res_p.size(0)\n",
    "            \n",
    "#             display_example(vocab, x[0], pred_z[0])\n",
    "            display_example(vocab, x[0], sent_rationale[0])\n",
    "            display_example(vocab, x[0], z[:,aspect,:][0])\n",
    "\n",
    "            prec = torch.sum(sent_rationale * sent_mask[:, :, 0], dim=1) / (torch.sum(sent_rationale, dim=1) + 1e-6)\n",
    "            rec = torch.sum(sent_rationale * sent_mask[:, :, 0], dim=1) / (torch.sum(sent_mask[:, :, 0], dim=1) + 1e-6)\n",
    "\n",
    "            z1_prec_sent += torch.sum(prec)\n",
    "            z1_rec_sent += torch.sum(rec)\n",
    "            z1_f1_sent += torch.sum(prec * rec * 2 / (prec + rec + 1e-6))\n",
    "\n",
    "            z1_total += z.size(0)\n",
    "           \n",
    "        if i_epoch < switch_epoch:\n",
    "            print (\"Test acc: %.4f\" % (correct_pred / num_sample))\n",
    "        else:\n",
    "            print (\"Test acc: %.4f sent-level: %.4f\" % (correct_pred / num_sample, \n",
    "                                                       correct_pred_sent / num_sample))\n",
    "#         for key in z_prec.keys():\n",
    "#             print(\"Top-%d: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (key, \n",
    "#                                                                       z_prec[key] / z_total, \n",
    "#                                                                       z_rec[key] / z_total, \n",
    "#                                                                       z_f1[key] / z_total))\n",
    "#         print(\"Sent-level: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (z_prec_sent / z_total, \n",
    "#                                                                       z_rec_sent / z_total, \n",
    "#                                                                       z_f1_sent / z_total))\n",
    "        for z_idx in range(5):\n",
    "            print(\"Sent-level: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (z_prec_sent[z_idx] / z_total_sent[z_idx], \n",
    "                                                                          z_rec_sent[z_idx] / z_total_sent[z_idx], \n",
    "                                                                          z_f1_sent[z_idx] / z_total_sent[z_idx]))\n",
    "\n",
    "        print(\"Z1 selection: Highlight precision: %.4f recall: %.4f f1: %.4f\" % (z1_prec_sent / z1_total, \n",
    "                                                                      z1_rec_sent / z1_total, \n",
    "                                                                      z1_f1_sent / z1_total))\n",
    "            \n",
    "        if i_epoch >= switch_epoch:\n",
    "            print (\"Token sparisity: %.4f\" % (dev_sparsity / num_sample)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_example(vocab, x[0], sent_rationale[0])\n",
    "# aspect = 2\n",
    "# display_example(vocab, x[0], z[:,aspect,:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
